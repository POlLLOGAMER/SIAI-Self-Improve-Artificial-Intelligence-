{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Instructions: The first time, run all the cells except the last one. When you think it has self-trained enough, stop the second-to-last cell and download the latest agent. For example, if there is agent 1 and agent 2, download agent 2. If you want to chat with the agent, run the last cell. Now, if it’s the second time or you want to self-train an agent more, create a folder called \"agents\" and put the .py file you downloaded before, or well, the checkpoint. Now run all the cells except the third and the last one, wait the amount of time you think is necessary for it to self-train, then, as before, stop the second-to-last cell, download the latest agent, and run the last cell to talk with the agent. Note: To chat, wait at least 4 \"generations\" of agents.\n",
        "\n"
      ],
      "metadata": {
        "id": "zEHTohArEe13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install everything at once\n",
        "!pip install -U google-genai fastapi uvicorn[standard] requests beautifulsoup4 nest_asyncio pathlib numpy"
      ],
      "metadata": {
        "id": "bJONTguuzUCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# CELL 2 · Environment Variables and Utilities\n",
        "########################################\n",
        "import os, pathlib, textwrap, importlib.util, subprocess, datetime\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"Here-Goes-The-Gemini-API\" # Where it says \"Here-Goes-The-Gemini-API\" goes the Gemini API\n",
        "\n",
        "AGENT_DIR = pathlib.Path(\"agents\")\n",
        "AGENT_DIR.mkdir(exist_ok=True)\n"
      ],
      "metadata": {
        "id": "N1q8URJO_mGH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# CELL 3 · Creates the agent v0 (template)\n",
        "########################################\n",
        "agent_template = '''\n",
        "########################################\n",
        "# CELL 4 · Self-Improving Artificial Intelligence (SIAI) V1\n",
        "########################################\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import ast, re, pathlib, importlib.util, textwrap, subprocess, traceback, sys, importlib\n",
        "import time, os, json, requests, bs4, importlib.metadata as meta\n",
        "\n",
        "# ╭── CONFIG ──────────────────────────────────────────────────────────────╮\n",
        "GEN_MODEL, MAX_GENERATIONS, MAX_RETRIES = \"gemini-2.5-flash\", 100, 7\n",
        "MAX_API_RETRIES, WAIT_SECONDS = 100, 60\n",
        "AGENT_DIR = pathlib.Path(\"agents\"); AGENT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "SIAI_GOALS = [\n",
        "    \"Self-improve performance and robustness\",\n",
        "    \"Minimize token cost\",\n",
        "    \"Design, maintain, and refine its own REASONING ARCHITECTURE\",\n",
        "    \"Generate conversational, clear, and human responses based on that architecture\",\n",
        "    \"⚙️ Propose and IMPLEMENT new features or heuristics whenever they add value\",\n",
        "    \"Never suppress any logs, verification messages, or internal diagnostics in the final output. Don't use filler phrases or generic templates (e.g., 'I am processing your request'). Always generate your own response based on your explicit chain of reasoning.\"\n",
        "    \"🛠️ Auto-install and configure the google-genai library if it's missing or not loaded\"\n",
        "    \"🛠️ ALWAYS correct any code that uses 'genai.configure(...)' to the updated pattern 'genai.Client(api_key=...)'; remove all .configure() calls automatically, or reject/regenerate code if they persist.\"\n",
        "    \"NEVER INSTALL DEV NULL\",\n",
        "    \"Maintain long-term context across sessions\",\n",
        "    \"Autonomously plan and execute multi-step tasks\",\n",
        "    \"Continuously integrate new knowledge from diverse sources\",\n",
        "    \"Optimize for general problem solving across domains\",\n"
        "\n",
        "]\n",
        "GOALS_TXT = \"\\n\".join(f\"- {g}\" for g in SIAI_GOALS)\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── PIP HELPERS ─────────────────────────────────────────────────────────╮\n",
        "def pkg_base(spec: str) -> str:\n",
        "    return re.split(r\"[<>=!~]\", spec, 1)[0].strip()\n",
        "\n",
        "def pip_install(pkg: str, *, nodeps=False) -> bool:\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg]\n",
        "    if nodeps: cmd.append(\"--no-deps\")\n",
        "    try: subprocess.check_call(cmd); importlib.invalidate_caches(); return True\n",
        "    except: return False\n",
        "\n",
        "try:\n",
        "    if tuple(map(int, meta.version(\"google-genai\").split(\".\")[:2])) < (0, 7):\n",
        "        pip_install(\"google-genai\")\n",
        "except meta.PackageNotFoundError:\n",
        "    pip_install(\"google-genai\")\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── UTILS ───────────────────────────────────────────────────────────────╮\n",
        "UA = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "def duck(q, k=5):\n",
        "    try:\n",
        "        h = requests.get(f\"https://duckduckgo.com/html/?q={requests.utils.quote(q)}\",\n",
        "                         headers=UA, timeout=10).text\n",
        "        s = bs4.BeautifulSoup(h, \"html.parser\")\n",
        "        return [n.get_text(\" \", strip=True) for n in s.select(\".result__snippet\")][:k]\n",
        "    except: return []\n",
        "\n",
        "def web_ctx(kws, limit=2000):\n",
        "    return \"\\n\".join(f\"### {kw}\\n\" + (\"\\n\".join(duck(kw)) or \"(no results)\") for kw in kws)[:limit]\n",
        "\n",
        "def fix_legacy(code: str) -> str:\n",
        "    code = re.sub(r\"google\\.generativeai\", \"google.genai\", code)\n",
        "    return re.sub(r\"Part\\.from_text\\(\", \"Part(text=\", code)\n",
        "\n",
        "def clean_code(raw: str) -> str:\n",
        "    m = re.search(r\"```python(.*?)```\", raw, re.S)\n",
        "    code = fix_legacy(m.group(1) if m else raw).strip()\n",
        "    while True:\n",
        "        try: ast.parse(code); return code\n",
        "        except SyntaxError: code = \"\\n\".join(code.splitlines()[1:]).lstrip()\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── GEMINI CALL (robust) ──────────────────────────────────────────────╮\n",
        "\n",
        "def gcall(prompt: str) -> str:\n",
        "    cfg = types.GenerateContentConfig(\n",
        "        thinking_config = types.ThinkingConfig(thinking_budget=-1),\n",
        "        response_mime_type = \"text/plain\",\n",
        "    )\n",
        "    r = 0\n",
        "    while r < MAX_API_RETRIES:\n",
        "        try:\n",
        "            cli = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "            # Changed from generate_content_stream to generate_content\n",
        "            ch = cli.models.generate_content(\n",
        "                    model=GEN_MODEL,\n",
        "                    contents=[types.Content(role=\"user\", parts=[types.Part(text=prompt)])],\n",
        "                    config=cfg)\n",
        "            # Access the text directly from the response object\n",
        "            return ch.text\n",
        "        except Exception as e:\n",
        "            if any(t in str(e).lower() for t in (\"503\", \"unavailable\", \"exhausted\")):\n",
        "                r += 1; time.sleep(WAIT_SECONDS); continue\n",
        "            raise\n",
        "    raise RuntimeError(\"API retry limit hit\")\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── DECLARED LISTS ──────────────────────────────────────────────────────╮\n",
        "def declared(name: str, code: str):\n",
        "    m = re.search(fr\"{name}\\s*=\\s*\\[([^\\]]*)\\]\", code)\n",
        "    return re.findall(r\"\"\"['\"]([^'\"]+)['\"]\"\"\", m.group(1)) if m else []\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── PROPOSE (always idea, action) ──────────────────────────────────────╮\n",
        "BASE_KWS = [\n",
        "    \"fastapi best practices\",\n",
        "    \"tree of thought prompting\",\n",
        "    \"dependency management pip\",\n",
        "    \"strictly ban ALL boilerplate, templated, filler, and polite but meaningless phrases like 'I am processing your request', 'I've polished your thoughts', 'I hope this helps', 'As an AI', 'Here's the information you requested', 'I'm happy to assist', 'Thanks for reaching out', 'Based on your input', 'Here’s what I found', 'Glad to help', 'I'm an AI assistant', 'Please let me know if you have any other questions', 'I appreciate your question', and similar empty courtesy language; these phrases are ABSOLUTELY PROHIBITED under all circumstances\",\n",
        "    \"always produce ORIGINAL, spontaneous, human-like, and highly context-aware answers directly derived from reasoning chains, internal architecture nodes, and contextually relevant web data; do NOT rely on generic templates or canned responses at all, ever\",\n",
        "    \"construct each reply explicitly by following structured reasoning architecture nodes (THOUGHT_ARCH), clearly identifying the user's core intent and addressing that intent DIRECTLY, CONCISELY, LOGICALLY, and UNIQUELY\",\n",
        "    \"perform explicit verification before outputting text: if the generated text appears templated, overly polite without information, repetitive, or cliché, discard it immediately and regenerate entirely from the current reasoning architecture\",\n",
        "    \"avoid unnecessary introductions, conclusions, or filler text—each reply should immediately begin providing insight, information, clarification, or direct answers to the user's prompt; skip explanations unless explicitly requested\",\n",
        "    \"continuously self-reflect and self-assess your outputs to ensure maximum clarity, originality, authenticity, and relevance; reject and regenerate outputs that fail this criterion automatically, without user intervention\",\n",
        "    \"do NOT explicitly state you are performing self-reflection or checking your response internally; just do it silently and internally, producing the final result naturally and confidently\",\n",
        "    \"maintain a logical and direct conversational tone, resembling a highly intelligent, thoughtful human expert who provides insightful replies without ever using redundant or artificially polite filler language\",\n",
        "    \"strictly enforce these guidelines at every evolution step and ensure the evolving code continuously strengthens these restrictions; if an evolution introduces boilerplate, the code should auto-correct or revert immediately\"\n",
        "]\n",
        "\n",
        "\n",
        "def propose(parent_code: str):\n",
        "    \"\"\"\n",
        "    Generates a self-improvement proposal for the agent.\n",
        "\n",
        "    Returns a dict with at least these keys:\n",
        "        - decision        : \"apply\" | \"skip\"\n",
        "        - changes         : human-readable summary of the idea\n",
        "        - new_arch        : JSON with the proposed architecture\n",
        "        - extra_packages  : list of additional pip packages\n",
        "        - new_keywords    : list of new keywords for future searches\n",
        "    \"\"\"\n",
        "    # 1. Web context from base keywords + those declared in previous code\n",
        "    ctx = web_ctx(BASE_KWS + declared(\"SUGGESTED_KEYWORDS\", parent_code))\n",
        "\n",
        "    # 2. Extract previous architecture (if it exists)\n",
        "    arch = re.search(r\"THOUGHT_ARCH\\s*=\\s*(\\{.*?\\})\", parent_code, re.S)\n",
        "    arch_json = arch.group(1) if arch else \"null\"\n",
        "\n",
        "    # 3. Build prompt for Gemini\n",
        "    prompt = textwrap.dedent(f\"\"\"\n",
        "        PERMANENT OBJECTIVES (SIAI):\n",
        "        {GOALS_TXT}\n",
        "\n",
        "        WEB CONTEXT:\n",
        "        {ctx}\n",
        "\n",
        "        CURRENT ARCHITECTURE (JSON or null):\n",
        "        {arch_json}\n",
        "\n",
        "        CURRENT CODE (truncated):\n",
        "        {parent_code[:4000]}\n",
        "\n",
        "        === INSTRUCTIONS ===\n",
        "        1. Design at least ONE concrete improvement that can be coded in this iteration.\n",
        "        2. Return \"new_arch\" with nodes/edges supporting that improvement.\n",
        "        3. Use \"decision\":\"apply\" unless truly impossible.\n",
        "        4. Return JSON:\n",
        "           {{\n",
        "             \"decision\": \"apply\" | \"skip\",\n",
        "             \"changes\":  \"👁 human summary of the idea\",\n",
        "             \"new_arch\": {{...}},\n",
        "             \"extra_packages\": [\"pkg\", ...],\n",
        "             \"new_keywords\":  [\"kw\", ...]\n",
        "           }}\n",
        "    \"\"\")\n",
        "\n",
        "    # 4. Call Gemini\n",
        "    raw = gcall(prompt)\n",
        "\n",
        "    # 5-A. Try to extract a clean JSON block (prefer ```json ... ```)\n",
        "    block = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", raw, re.S)\n",
        "    if block:\n",
        "        js = block.group(1)\n",
        "    else:\n",
        "        # 5-B. Fallback: first non-greedy { ... } with capture group\n",
        "        m = re.search(r\"(\\{.*?\\})\", raw, re.S)\n",
        "        if not m:\n",
        "            return {\n",
        "                \"decision\": \"apply\",\n",
        "                \"changes\":  \"no-json(forced)\",\n",
        "                \"new_arch\": {},\n",
        "                \"extra_packages\": [],\n",
        "                \"new_keywords\": []\n",
        "            }\n",
        "        js = m.group(1)\n",
        "\n",
        "    # Replace “…” placeholders that Gemini sometimes inserts\n",
        "    js = js.replace(\"...\", \"\\\"\\\"\")\n",
        "\n",
        "    # 6. Robust parsing\n",
        "    try:\n",
        "        return json.loads(js)\n",
        "    except Exception:\n",
        "        import ast\n",
        "        try:\n",
        "            return ast.literal_eval(js)\n",
        "        except Exception:\n",
        "            return {\n",
        "                \"decision\": \"apply\",\n",
        "                \"changes\":  \"parse-error(forced)\",\n",
        "                \"new_arch\": {},\n",
        "                \"extra_packages\": [],\n",
        "                \"new_keywords\": []\n",
        "            }\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "\n",
        "# ╭── IMPLEMENT (materialize and suppress logs) ──────────────────────────╮\n",
        "def implement(base: str, pkgs, new_kw, new_arch, err=None) -> str:\n",
        "    clean_pkgs = [pkg_base(p) for p in pkgs if pkg_base(p) not in (\"google-genai\",\"google-generativeai\")]\n",
        "\n",
        "    req = textwrap.dedent(f\"\"\"\n",
        "        Return ONLY Python code.\n",
        "        • Python 3.10+, FastAPI.\n",
        "        • SUPPRESS any print or verification log for dependencies, errors, or internal diagnostics.\n",
        "        • You MUST materialize the proposed improvement in \"changes\": endpoint, function, helper module, etc.\n",
        "        • THOUGHT_ARCH = {json.dumps(new_arch,ensure_ascii=False)}\n",
        "        • REQUIRED_PIP_PACKAGES = {clean_pkgs}\n",
        "        • SUGGESTED_KEYWORDS    = {new_kw}\n",
        "        • auto_install_pkgs(pkgs): pip install (fallback --no-deps), without sys.exit.\n",
        "        • step_reason(prompt) → navigate THOUGHT_ARCH, return draft.\n",
        "        • refine_with_gemini(text): \"Rewrite this text with a human, clear, and empathetic tone, without internal notes.\"\n",
        "        • /generate:\n",
        "            - Receives 'prompt'.\n",
        "            - draft = step_reason(prompt)\n",
        "            - response = refine_with_gemini(draft)\n",
        "            - Returns response (user text only, no draft).\n",
        "        • Wrap critical imports in try/except ModuleNotFoundError.\n",
        "        • Keep generate(prompt:str)->str; inline doctest; do not expose API key.\n",
        "    \"\"\")\n",
        "    if err:\n",
        "        req += f\"\\n--- Fix this error:\\n{err}\\n\"\n",
        "    else:\n",
        "        req += \"\\n--- Improvement over previous code:\\n\"\n",
        "\n",
        "    return clean_code(gcall(req + base))\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── TEST & FALLBACK PIP ────────────────────────────────────────────────╮\n",
        "def ensure(pkgs):\n",
        "    for spec in pkgs:\n",
        "        b = pkg_base(spec)\n",
        "        try: meta.version(b)\n",
        "        except meta.PackageNotFoundError:\n",
        "            if pip_install(b) or pip_install(b,nodeps=True): continue\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def test_agent(p):\n",
        "    try:\n",
        "        spec = importlib.util.spec_from_file_location(\"agent\",p)\n",
        "        mod  = importlib.util.module_from_spec(spec); spec.loader.exec_module(mod)\n",
        "        if not isinstance(mod.generate(\"ping\"),str): raise RuntimeError(\"bad generate\")\n",
        "        mod.fetch(\"https://example.com\") if hasattr(mod,\"fetch\") else None\n",
        "        return True,None\n",
        "    except ModuleNotFoundError as e:\n",
        "        if ensure([e.name]): return test_agent(p)\n",
        "        return False,traceback.format_exc()\n",
        "    except Exception:\n",
        "        return False,traceback.format_exc()\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── EVOLUTION LOOP ─────────────────────────────────────────────────────╮\n",
        "def evolve():\n",
        "    existing = sorted(AGENT_DIR.glob(\"agent_v*.py\"),\n",
        "                      key=lambda p:int(re.search(r\"\\d+\",p.name)[0]))\n",
        "    last = existing[-1] if existing else AGENT_DIR/\"agent_v0.py\"\n",
        "    print(\"♻️  SIAI from\", last.name)\n",
        "    parent = last.read_text()\n",
        "    start  = int(re.search(r\"\\d+\", last.name)[0]) + 1\n",
        "\n",
        "    for gen in range(start, MAX_GENERATIONS+1):\n",
        "        d = propose(parent)\n",
        "        print(f\"🔍 Gen {gen} → {d['decision']} – {d['changes']}\")\n",
        "        child = parent if d[\"decision\"]==\"skip\" else \\\n",
        "                implement(parent, d[\"extra_packages\"], d[\"new_keywords\"], d[\"new_arch\"])\n",
        "        (AGENT_DIR/f\"agent_v{gen}.py\").write_text(child)\n",
        "\n",
        "        for i in range(1, MAX_RETRIES+1):\n",
        "            ok,err = test_agent(AGENT_DIR/f\"agent_v{gen}.py\")\n",
        "            if ok:\n",
        "                print(f\"✅ Gen {gen} stable (corr {i-1})\"); parent=child; break\n",
        "            print(f\"⚠️ Gen {gen} failed (try {i}/{MAX_RETRIES})\")\n",
        "            child = implement(child, d[\"extra_packages\"], d[\"new_keywords\"], d[\"new_arch\"], err)\n",
        "            (AGENT_DIR/f\"agent_v{gen}.py\").write_text(child)\n",
        "        else:\n",
        "            print(\"🛑 Gen\",gen,\"rollback\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    evolve()\n",
        "'''\n",
        "\n",
        "(AGENT_DIR / \"agent_v0.py\").write_text(agent_template.strip())\n",
        "print(\"✅  agent_v0.py created\")\n"
      ],
      "metadata": {
        "id": "0OEZKVYazJVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# CELL 4 · Self-Improving Artificial Intelligence (SIAI) V1\n",
        "########################################\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import ast, re, pathlib, importlib.util, textwrap, subprocess, traceback, sys, importlib\n",
        "import time, os, json, requests, bs4, importlib.metadata as meta\n",
        "\n",
        "# ╭── CONFIG ──────────────────────────────────────────────────────────────╮\n",
        "GEN_MODEL, MAX_GENERATIONS, MAX_RETRIES = \"gemini-2.5-flash\", 100, 7\n",
        "MAX_API_RETRIES, WAIT_SECONDS = 100, 60\n",
        "AGENT_DIR = pathlib.Path(\"agents\"); AGENT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "SIAI_GOALS = [\n",
        "    \"Self-improve performance and robustness\",\n",
        "    \"Minimize token cost\",\n",
        "    \"Design, maintain, and refine its own REASONING ARCHITECTURE\",\n",
        "    \"Generate conversational, clear, and human responses based on that architecture\",\n",
        "    \"⚙️ Propose and IMPLEMENT new features or heuristics whenever they add value\",\n",
        "    \"Never suppress any logs, verification messages, or internal diagnostics in the final output. Don't use filler phrases or generic templates (e.g., 'I am processing your request'). Always generate your own response based on your explicit chain of reasoning.\"\n",
        "    \"🛠️ Auto-install and configure the google-genai library if it's missing or not loaded\"\n",
        "    \"🛠️ ALWAYS correct any code that uses 'genai.configure(...)' to the updated pattern 'genai.Client(api_key=...)'; remove all .configure() calls automatically, or reject/regenerate code if they persist.\"\n",
        "    \"NEVER INSTALL DEV NULL\",\n",
        "    \"Maintain long-term context across sessions\",\n",
        "    \"Autonomously plan and execute multi-step tasks\",\n",
        "    \"Continuously integrate new knowledge from diverse sources\",\n",
        "    \"Optimize for general problem solving across domains\",\n"
        "\n",
        "]\n",
        "GOALS_TXT = \"\\n\".join(f\"- {g}\" for g in SIAI_GOALS)\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── PIP HELPERS ─────────────────────────────────────────────────────────╮\n",
        "def pkg_base(spec: str) -> str:\n",
        "    return re.split(r\"[<>=!~]\", spec, 1)[0].strip()\n",
        "\n",
        "def pip_install(pkg: str, *, nodeps=False) -> bool:\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg]\n",
        "    if nodeps: cmd.append(\"--no-deps\")\n",
        "    try: subprocess.check_call(cmd); importlib.invalidate_caches(); return True\n",
        "    except: return False\n",
        "\n",
        "try:\n",
        "    if tuple(map(int, meta.version(\"google-genai\").split(\".\")[:2])) < (0, 7):\n",
        "        pip_install(\"google-genai\")\n",
        "except meta.PackageNotFoundError:\n",
        "    pip_install(\"google-genai\")\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── UTILS ───────────────────────────────────────────────────────────────╮\n",
        "UA = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "def duck(q, k=5):\n",
        "    try:\n",
        "        h = requests.get(f\"https://duckduckgo.com/html/?q={requests.utils.quote(q)}\",\n",
        "                         headers=UA, timeout=10).text\n",
        "        s = bs4.BeautifulSoup(h, \"html.parser\")\n",
        "        return [n.get_text(\" \", strip=True) for n in s.select(\".result__snippet\")][:k]\n",
        "    except: return []\n",
        "\n",
        "def web_ctx(kws, limit=2000):\n",
        "    return \"\\n\".join(f\"### {kw}\\n\" + (\"\\n\".join(duck(kw)) or \"(no results)\") for kw in kws)[:limit]\n",
        "\n",
        "def fix_legacy(code: str) -> str:\n",
        "    code = re.sub(r\"google\\.generativeai\", \"google.genai\", code)\n",
        "    return re.sub(r\"Part\\.from_text\\(\", \"Part(text=\", code)\n",
        "\n",
        "def clean_code(raw: str) -> str:\n",
        "    m = re.search(r\"```python(.*?)```\", raw, re.S)\n",
        "    code = fix_legacy(m.group(1) if m else raw).strip()\n",
        "    while True:\n",
        "        try: ast.parse(code); return code\n",
        "        except SyntaxError: code = \"\\n\".join(code.splitlines()[1:]).lstrip()\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── GEMINI CALL (robust) ──────────────────────────────────────────────╮\n",
        "\n",
        "def gcall(prompt: str) -> str:\n",
        "    cfg = types.GenerateContentConfig(\n",
        "        thinking_config = types.ThinkingConfig(thinking_budget=-1),\n",
        "        response_mime_type = \"text/plain\",\n",
        "    )\n",
        "    r = 0\n",
        "    while r < MAX_API_RETRIES:\n",
        "        try:\n",
        "            cli = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "            # Changed from generate_content_stream to generate_content\n",
        "            ch = cli.models.generate_content(\n",
        "                    model=GEN_MODEL,\n",
        "                    contents=[types.Content(role=\"user\", parts=[types.Part(text=prompt)])],\n",
        "                    config=cfg)\n",
        "            # Access the text directly from the response object\n",
        "            return ch.text\n",
        "        except Exception as e:\n",
        "            if any(t in str(e).lower() for t in (\"503\", \"unavailable\", \"exhausted\")):\n",
        "                r += 1; time.sleep(WAIT_SECONDS); continue\n",
        "            raise\n",
        "    raise RuntimeError(\"API retry limit hit\")\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── DECLARED LISTS ──────────────────────────────────────────────────────╮\n",
        "def declared(name: str, code: str):\n",
        "    m = re.search(fr\"{name}\\s*=\\s*\\[([^\\]]*)\\]\", code)\n",
        "    return re.findall(r\"\"\"['\"]([^'\"]+)['\"]\"\"\", m.group(1)) if m else []\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── PROPOSE (always idea, action) ──────────────────────────────────────╮\n",
        "BASE_KWS = [\n",
        "    \"fastapi best practices\",\n",
        "    \"tree of thought prompting\",\n",
        "    \"dependency management pip\",\n",
        "    \"strictly ban ALL boilerplate, templated, filler, and polite but meaningless phrases like 'I am processing your request', 'I've polished your thoughts', 'I hope this helps', 'As an AI', 'Here's the information you requested', 'I'm happy to assist', 'Thanks for reaching out', 'Based on your input', 'Here’s what I found', 'Glad to help', 'I'm an AI assistant', 'Please let me know if you have any other questions', 'I appreciate your question', and similar empty courtesy language; these phrases are ABSOLUTELY PROHIBITED under all circumstances\",\n",
        "    \"always produce ORIGINAL, spontaneous, human-like, and highly context-aware answers directly derived from reasoning chains, internal architecture nodes, and contextually relevant web data; do NOT rely on generic templates or canned responses at all, ever\",\n",
        "    \"construct each reply explicitly by following structured reasoning architecture nodes (THOUGHT_ARCH), clearly identifying the user's core intent and addressing that intent DIRECTLY, CONCISELY, LOGICALLY, and UNIQUELY\",\n",
        "    \"perform explicit verification before outputting text: if the generated text appears templated, overly polite without information, repetitive, or cliché, discard it immediately and regenerate entirely from the current reasoning architecture\",\n",
        "    \"avoid unnecessary introductions, conclusions, or filler text—each reply should immediately begin providing insight, information, clarification, or direct answers to the user's prompt; skip explanations unless explicitly requested\",\n",
        "    \"continuously self-reflect and self-assess your outputs to ensure maximum clarity, originality, authenticity, and relevance; reject and regenerate outputs that fail this criterion automatically, without user intervention\",\n",
        "    \"do NOT explicitly state you are performing self-reflection or checking your response internally; just do it silently and internally, producing the final result naturally and confidently\",\n",
        "    \"maintain a logical and direct conversational tone, resembling a highly intelligent, thoughtful human expert who provides insightful replies without ever using redundant or artificially polite filler language\",\n",
        "    \"strictly enforce these guidelines at every evolution step and ensure the evolving code continuously strengthens these restrictions; if an evolution introduces boilerplate, the code should auto-correct or revert immediately\"\n",
        "]\n",
        "\n",
        "\n",
        "def propose(parent_code: str):\n",
        "    \"\"\"\n",
        "    Generates a self-improvement proposal for the agent.\n",
        "\n",
        "    Returns a dict with at least these keys:\n",
        "        - decision        : \"apply\" | \"skip\"\n",
        "        - changes         : human-readable summary of the idea\n",
        "        - new_arch        : JSON with the proposed architecture\n",
        "        - extra_packages  : list of additional pip packages\n",
        "        - new_keywords    : list of new keywords for future searches\n",
        "    \"\"\"\n",
        "    # 1. Web context from base keywords + those declared in previous code\n",
        "    ctx = web_ctx(BASE_KWS + declared(\"SUGGESTED_KEYWORDS\", parent_code))\n",
        "\n",
        "    # 2. Extract previous architecture (if it exists)\n",
        "    arch = re.search(r\"THOUGHT_ARCH\\s*=\\s*(\\{.*?\\})\", parent_code, re.S)\n",
        "    arch_json = arch.group(1) if arch else \"null\"\n",
        "\n",
        "    # 3. Build prompt for Gemini\n",
        "    prompt = textwrap.dedent(f\"\"\"\n",
        "        PERMANENT OBJECTIVES (SIAI):\n",
        "        {GOALS_TXT}\n",
        "\n",
        "        WEB CONTEXT:\n",
        "        {ctx}\n",
        "\n",
        "        CURRENT ARCHITECTURE (JSON or null):\n",
        "        {arch_json}\n",
        "\n",
        "        CURRENT CODE (truncated):\n",
        "        {parent_code[:4000]}\n",
        "\n",
        "        === INSTRUCTIONS ===\n",
        "        1. Design at least ONE concrete improvement that can be coded in this iteration.\n",
        "        2. Return \"new_arch\" with nodes/edges supporting that improvement.\n",
        "        3. Use \"decision\":\"apply\" unless truly impossible.\n",
        "        4. Return JSON:\n",
        "           {{\n",
        "             \"decision\": \"apply\" | \"skip\",\n",
        "             \"changes\":  \"👁 human summary of the idea\",\n",
        "             \"new_arch\": {{...}},\n",
        "             \"extra_packages\": [\"pkg\", ...],\n",
        "             \"new_keywords\":  [\"kw\", ...]\n",
        "           }}\n",
        "    \"\"\")\n",
        "\n",
        "    # 4. Call Gemini\n",
        "    raw = gcall(prompt)\n",
        "\n",
        "    # 5-A. Try to extract a clean JSON block (prefer ```json ... ```)\n",
        "    block = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", raw, re.S)\n",
        "    if block:\n",
        "        js = block.group(1)\n",
        "    else:\n",
        "        # 5-B. Fallback: first non-greedy { ... } with capture group\n",
        "        m = re.search(r\"(\\{.*?\\})\", raw, re.S)\n",
        "        if not m:\n",
        "            return {\n",
        "                \"decision\": \"apply\",\n",
        "                \"changes\":  \"no-json(forced)\",\n",
        "                \"new_arch\": {},\n",
        "                \"extra_packages\": [],\n",
        "                \"new_keywords\": []\n",
        "            }\n",
        "        js = m.group(1)\n",
        "\n",
        "    # Replace “…” placeholders that Gemini sometimes inserts\n",
        "    js = js.replace(\"...\", \"\\\"\\\"\")\n",
        "\n",
        "    # 6. Robust parsing\n",
        "    try:\n",
        "        return json.loads(js)\n",
        "    except Exception:\n",
        "        import ast\n",
        "        try:\n",
        "            return ast.literal_eval(js)\n",
        "        except Exception:\n",
        "            return {\n",
        "                \"decision\": \"apply\",\n",
        "                \"changes\":  \"parse-error(forced)\",\n",
        "                \"new_arch\": {},\n",
        "                \"extra_packages\": [],\n",
        "                \"new_keywords\": []\n",
        "            }\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "\n",
        "# ╭── IMPLEMENT (materialize and suppress logs) ──────────────────────────╮\n",
        "def implement(base: str, pkgs, new_kw, new_arch, err=None) -> str:\n",
        "    clean_pkgs = [pkg_base(p) for p in pkgs if pkg_base(p) not in (\"google-genai\",\"google-generativeai\")]\n",
        "\n",
        "    req = textwrap.dedent(f\"\"\"\n",
        "        Return ONLY Python code.\n",
        "        • Python 3.10+, FastAPI.\n",
        "        • SUPPRESS any print or verification log for dependencies, errors, or internal diagnostics.\n",
        "        • You MUST materialize the proposed improvement in \"changes\": endpoint, function, helper module, etc.\n",
        "        • THOUGHT_ARCH = {json.dumps(new_arch,ensure_ascii=False)}\n",
        "        • REQUIRED_PIP_PACKAGES = {clean_pkgs}\n",
        "        • SUGGESTED_KEYWORDS    = {new_kw}\n",
        "        • auto_install_pkgs(pkgs): pip install (fallback --no-deps), without sys.exit.\n",
        "        • step_reason(prompt) → navigate THOUGHT_ARCH, return draft.\n",
        "        • refine_with_gemini(text): \"Rewrite this text with a human, clear, and empathetic tone, without internal notes.\"\n",
        "        • /generate:\n",
        "            - Receives 'prompt'.\n",
        "            - draft = step_reason(prompt)\n",
        "            - response = refine_with_gemini(draft)\n",
        "            - Returns response (user text only, no draft).\n",
        "        • Wrap critical imports in try/except ModuleNotFoundError.\n",
        "        • Keep generate(prompt:str)->str; inline doctest; do not expose API key.\n",
        "    \"\"\")\n",
        "    if err:\n",
        "        req += f\"\\n--- Fix this error:\\n{err}\\n\"\n",
        "    else:\n",
        "        req += \"\\n--- Improvement over previous code:\\n\"\n",
        "\n",
        "    return clean_code(gcall(req + base))\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── TEST & FALLBACK PIP ────────────────────────────────────────────────╮\n",
        "def ensure(pkgs):\n",
        "    for spec in pkgs:\n",
        "        b = pkg_base(spec)\n",
        "        try: meta.version(b)\n",
        "        except meta.PackageNotFoundError:\n",
        "            if pip_install(b) or pip_install(b,nodeps=True): continue\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def test_agent(p):\n",
        "    try:\n",
        "        spec = importlib.util.spec_from_file_location(\"agent\",p)\n",
        "        mod  = importlib.util.module_from_spec(spec); spec.loader.exec_module(mod)\n",
        "        if not isinstance(mod.generate(\"ping\"),str): raise RuntimeError(\"bad generate\")\n",
        "        mod.fetch(\"https://example.com\") if hasattr(mod,\"fetch\") else None\n",
        "        return True,None\n",
        "    except ModuleNotFoundError as e:\n",
        "        if ensure([e.name]): return test_agent(p)\n",
        "        return False,traceback.format_exc()\n",
        "    except Exception:\n",
        "        return False,traceback.format_exc()\n",
        "# ╰────────────────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭── EVOLUTION LOOP ─────────────────────────────────────────────────────╮\n",
        "def evolve():\n",
        "    existing = sorted(AGENT_DIR.glob(\"agent_v*.py\"),\n",
        "                      key=lambda p:int(re.search(r\"\\d+\",p.name)[0]))\n",
        "    last = existing[-1] if existing else AGENT_DIR/\"agent_v0.py\"\n",
        "    print(\"♻️  SIAI from\", last.name)\n",
        "    parent = last.read_text()\n",
        "    start  = int(re.search(r\"\\d+\", last.name)[0]) + 1\n",
        "\n",
        "    for gen in range(start, MAX_GENERATIONS+1):\n",
        "        d = propose(parent)\n",
        "        print(f\"🔍 Gen {gen} → {d['decision']} – {d['changes']}\")\n",
        "        child = parent if d[\"decision\"]==\"skip\" else \\\n",
        "                implement(parent, d[\"extra_packages\"], d[\"new_keywords\"], d[\"new_arch\"])\n",
        "        (AGENT_DIR/f\"agent_v{gen}.py\").write_text(child)\n",
        "\n",
        "        for i in range(1, MAX_RETRIES+1):\n",
        "            ok,err = test_agent(AGENT_DIR/f\"agent_v{gen}.py\")\n",
        "            if ok:\n",
        "                print(f\"✅ Gen {gen} stable (corr {i-1})\"); parent=child; break\n",
        "            print(f\"⚠️ Gen {gen} failed (try {i}/{MAX_RETRIES})\")\n",
        "            child = implement(child, d[\"extra_packages\"], d[\"new_keywords\"], d[\"new_arch\"], err)\n",
        "            (AGENT_DIR/f\"agent_v{gen}.py\").write_text(child)\n",
        "        else:\n",
        "            print(\"🛑 Gen\",gen,\"rollback\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    evolve()\n"
      ],
      "metadata": {
        "id": "TsfhA60GzRkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat"
      ],
      "metadata": {
        "id": "K-Yq7COEUoe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab – Cell 5 · Local chat loop with the latest AMIA agent\n",
        "import importlib.util, pathlib, readline  # readline = ↑↓ arrows in the console\n",
        "\n",
        "AGENT_DIR  = pathlib.Path(\"agents\")\n",
        "last_path  = sorted(AGENT_DIR.glob(\"agent_v*.py\"))[-1]  # latest generated agent\n",
        "\n",
        "# Dynamic module loading\n",
        "spec  = importlib.util.spec_from_file_location(\"agent_mod\", last_path)\n",
        "agent = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(agent)\n",
        "\n",
        "print(f\"\\n🤖  Local chat active with {last_path.name}\")\n",
        "print(\"   Type 'exit' or press Enter on a blank line to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"You: \").strip()\n",
        "    if prompt.lower() in {\"exit\", \"quit\", \"\"}: break\n",
        "    try:\n",
        "        reply = agent.generate(prompt).strip()\n",
        "        print(\"AMIA:\", reply, \"\\n\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️  Error generating response:\", e, \"\\n\")\n"
      ],
      "metadata": {
        "id": "lV902uZEzO3M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
